<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiaofeng Wang</title>
  
  <meta name="author" content="Xiaofeng Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaofeng Wang</name>
              </p>
              <p>
              I am currently in my fourth year as a Ph.D. student at the Institute of Automation, Chinese Academy of Sciences (CASIA). Prior to that, I received my Bachelor's degree from the department of Automation, Nanjing University of Science and Technology (NJUST) in 2020. Additionally, I have spent some time at University of Dayton (UD), <a href="https://research.megvii.com/">Megvii</a>, <a href="https://www.phigent.ai/#/">PhiGent</a>, and <a href="https://studio.gigaai.cc/">GigaAI</a>.
              </p>  
              <p>
              My research interests revolve around <strong>general world models</strong> and <strong>3D perceptions</strong>, aiming to develop understanding of physics and motion in AI systems. I aspire to contribute to advancements in autonomous driving and embodied AI applications through my research. Please feel free to reach out if you have any questions or would like to discuss further.
              </p>
              <p style="text-align:center">
                <a href="mailto:wangxiaofeng2020@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=96lsfiUAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/JeffWang987"> Github </a> 
                <!-- &nbsp/&nbsp -->
                <!-- <a href="data/CV_Xiaofeng_Wang.pdf">Curriculum Vitae</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="images/profile.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2023-11:</b> One <a href="https://arxiv.org/pdf/2311.05332.pdf">technique report</a> exploring GPT-4V on autonomous driving is available. Exciting to see the <a href="https://twitter.com/gdb/status/1724094850788688023">community sharing thoughts</a> on our latest findings!
              <li style="margin: 5px;" >
                <b>2023-07:</b> One paper on 3D occupancy prediction is accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
              <li style="margin: 5px;" >
                <b>2023-02:</b> One paper on 3D streaming perception is accepted to <a href="http://cvpr2023.thecvf.com/">CVPR 2023</a>.
                <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on 3D pretraining is accepted to <a href="http://cvpr2023.thecvf.com/">ICLR 2023</a>.
              <li style="margin: 5px;" >
                <b>2022-11:</b> One paper on self-supervised depth estimation is accepted to <a href="https://aaai-23.aaai.org/">AAAI 2023</a>.
              <li style="margin: 5px;" >
                <b>2022-07:</b> One paper on multi-view depth estimation is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GPT4V.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving</papertitle>
              <br>
              <a href="https://github.com/zijinoier"> Licheng Wen</a>*,  <a href="https://scholar.google.com/citations?user=xGuZsikAAAAJ&hl=en"> Xuemeng Yang</a>*, <a href="https://github.com/Fdarco"> Daocheng Fu</a>*, <strong>Xiaofeng Wang</strong>*, <a href="https://pinlong-cai.github.io/">Pinlong Cai</a>, <a href="https://scholar.google.com/citations?user=7atts2cAAAAJ&hl=zh-CN">Xin Li</a>, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en">Zheng Zhu</a>, Shaoyan Sun, <a href="https://scholar.google.com.sg/citations?user=_fl950wAAAAJ&hl=en">Yeqi Bai</a>, <a href="https://scholar.google.com/citations?user=v9CvBeUAAAAJ&hl=en">Xinyu Cai</a>, Min Dou, Shuanglu Hu, <a href="https://scholar.google.com/citations?user=K0PpvLkAAAAJ&hl=en">Botian Shi</a>, <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Yu Qiao</a>
              <br>
              <em>arXiv, 2023
              <br>
              <a href="https://arxiv.org/abs/2311.05332">[arXiv]</a> <a href="https://github.com/PJLab-ADG/GPT4V-AD-Exploration">[Page]</a>
              <br>
              <p> This report provides an exhaustive evaluation of the latest state-of-the-art VLM, GPT-4V(ision), and its application in autonomous driving scenarios. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/drivedreamer.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Drivedreamer: Towards real-world-driven world models for autonomous driving</papertitle>
              <br>
              <strong>Xiaofeng Wang</strong>*,  <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en"> Zheng Zhu</a>*,  Guan Huang, Xinze Chen, Jiagang Zhu, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>arXiv, 2023
              <br>
              <a href="https://arxiv.org/pdf/2309.09777.pdf">[arXiv]</a> <a href="https://drivedreamer.github.io/">[page]</a> <a href="https://github.com/JeffWang987/DriveDreamer">[code]</a>
              <br>
              <p> DriveDreamer is the first world model established from real-world driving scenarios. It empowers controllable driving video generation and enables the prediction of reasonable driving policies. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/openoccupancy.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception</papertitle>
              <br>
              <strong>Xiaofeng Wang</strong>*,  <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en"> Zheng Zhu</a>*,  Wenbo Xu*, <a href="https://github.com/zhangyp15"> Yunpeng Zhang</a>, <a href="https://github.com/weiyithu"> Yi Wei</a>, Xu Chi, Yun Ye, Dalong Du,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://ieeexplore.ieee.org/author/37407464000"> Xingang Wang </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.03991.pdf">[arXiv]</a> <a href="https://github.com/JeffWang987/OpenOccupancy">[Code]</a>
              <br>
              <p> Towards a comprehensive benchmarking of surrounding perception algorithms, we propose OpenOccupancy, which is the first surrounding semantic occupancy perception benchmark. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/stereoscene.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</papertitle>
              <br>
              <a href="https://github.com/Arlo0o">Bohan Li</a> ,<a href="https://scholar.google.com/citations?user=Vrq1yOEAAAAJ&hl=en">Yasheng Sun</a>, <a href="https://scholar.google.com/citations?user=byaSC-kAAAAJ&hl=zh-CN">Xin Jin</a>, <a href="https://scholar.google.com/citations?user=_cUfvYQAAAAJ&hl=zh-CN">Wenjun Zeng</a>, <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en"> Zheng Zhu</a>, <strong>Xiaofeng Wang</strong>, <a href="https://github.com/zhangyp15"> Yunpeng Zhang</a>, <a href="https://scholar.google.com/citations?user=RD6CLmUAAAAJ&hl=en">James Okae</a>, Hang Xiao, Dalong Du
              <br>
              <em>arXiv, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.13959">[arXiv]</a> <a href="https://github.com/Arlo0o/StereoScene">[Code]</a>
              <br>
              <p> We propose StereoScene for 3D Semantic Scene Completion (SSC), which explores taking full advantage of light-weight camera inputs without resorting to any external 3D sensors. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/asap.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark</papertitle>
              <br>
              <strong>Xiaofeng Wang</strong>,  <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en"> Zheng Zhu</a>,  <a href="https://github.com/zhangyp15"> Yunpeng Zhang</a>, Guan Huang, Yun Ye, Wenbo Xu, <a href="https://github.com/RichardChen20"> Ziwei Chen </a>, <a href="https://ieeexplore.ieee.org/author/37407464000"> Xingang Wang </a>
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recogintion (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2212.08914.pdf">[arXiv]</a> <a href="https://github.com/JeffWang987/ASAP">[Code]</a>
              <br>
              <p> We propose the Autonomousdriving StreAming Perception (ASAP) benchmark, which is the first benchmark to evaluate the online performance of vision-centric perception in autonomous driving. </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/liftcl.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LiftedCL: Lifting Contrastive Learning for Human-Centric Perception</papertitle>
              <br>
              <a href="https://github.com/RichardChen20"> Ziwei Chen </a>,  <a href="https://scholar.google.com.hk/citations?user=GGPvOP4AAAAJ&hl=zh-CN"> Qiang Li </a>, <strong>Xiaofeng Wang</strong>, <a href="https://scholar.google.com/citations?user=inPYAuYAAAAJ&hl=zh-CN">Wankou Yang</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://openreview.net/pdf?id=WHlt5tLz12T">[paper]</a> <a href="https://richardchen20.github.io/LiftedCL/">[page]</a> <a href="https://github.com/RichardChen20/LiftedCL">[code]</a>
              <br>
              <p> We propose the Lifting Contrastive Learning (LiftedCL) to obtain 3D-aware human-centric representations which absorb 3D human structure information. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/movedepth.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MOVEDepth: Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning</papertitle>
              <br>
              <strong>Xiaofeng Wang</strong>,  <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en"> Zheng Zhu</a>, Guan Huang, Xu Chi, Yun Ye, <a href="https://github.com/RichardChen20"> Ziwei Chen </a>, <a href="https://ieeexplore.ieee.org/author/37407464000"> Xingang Wang </a>
              <br>
              <em>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2208.09170">[arXiv]</a> <a href="https://github.com/JeffWang987/MOVEDepth">[code]</a>
              <br>
              <p> MOVEDepth is a self-supervised depth estimation method that explores monocular cues to enhance the multi-frame depth learning. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/mvster.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MVSTER: Epipolar Transformer for Efficient Multi-View Stereo</papertitle>
              <br>
              <strong>Xiaofeng Wang</strong>,  <a href="https://scholar.google.com/citations?user=NmwjI0AAAAAJ&hl=en"> Zheng Zhu</a>, <a href="https://scholar.google.com/citations?user=ZOpvcr4AAAAJ&hl=en">Fangbo Qin</a>, Yun Ye, Guan Huang, Xu Chi, <a href="https://scholar.google.com/citations?user=_0lKGnkAAAAJ&hl=en"> Yijia He </a>, <a href="https://ieeexplore.ieee.org/author/37407464000"> Xingang Wang </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.07346">[arXiv]</a> <a href="https://github.com/JeffWang987/MVSTER">[code]</a>
              <br>
              <p> We propose a novel end-to-end Transformer-based method for multi-view stereo, named MVSTER. It leverages the proposed epipolar Transformer to efficiently learn 3D associations along epipolar line. </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2019 Ruihua Cup Annual Outstanding College Student / 瑞华杯大学生年度人物 </li>
                <li style="margin: 5px;"> 2019 President's Medal of NJUST / 校长奖章 </li>
                <li style="margin: 5px;"> 2019 National Scholarship / 国家奖学金 </li>
                <li style="margin: 5px;"> 2018 CSC Scholarship / 国家公派留学奖学金 </li>
                <li style="margin: 5px;"> 2017 National Scholarship / 国家奖学金 </li>
              </p>
            </td>
          </tr>
        </tbody></table>

       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=_digsn9JgHMCzNIVQA7a_ncS2LnTef831szKg1uKJeg"></script>
	  </div>        
	  <br>
	    &copy; Xiaofeng Wang | Last updated: Dec 15, 2023
</center></p>
</body>

</html>
